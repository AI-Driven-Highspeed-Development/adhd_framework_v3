{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82f1181",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import the tokenizer and configure paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "200fd346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer imported successfully\n",
      "Available token types: ['COMMENT', 'IMPORT', 'NODE_DEF', 'NODE_REF', 'FORWARD_REF', 'FILE_REF', 'PIPE', 'DOT_END', 'ASSIGN', 'STRING_TRIM', 'STRING_PRESERVE', 'IDENTIFIER', 'PARAM_NAME', 'EOF']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/stellar/PublicRepo/YMHY/anime_streamer_2_0')\n",
    "\n",
    "from cores.flow_core.tokenizer import Tokenizer\n",
    "from cores.flow_core.models import TokenType, Token\n",
    "\n",
    "# Create a tokenizer instance\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "print(\"✓ Tokenizer imported successfully\")\n",
    "print(f\"Available token types: {[t.name for t in TokenType]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0504a38",
   "metadata": {},
   "source": [
    "## 2. Basic Tokenization\n",
    "\n",
    "A simple example showing how source text becomes tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c57f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer instance created successfully\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "print(\"✓ Tokenizer instance created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300e5091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 52 chars -> 9 tokens\n",
      "\n",
      "  NODE_DEF        | 'greeting'                     | line 2, col 1\n",
      "  PIPE            | '|'                            | line 2, col 11\n",
      "  STRING_TRIM     | 'Hello, World!'                | line 2, col 12\n",
      "  DOT_END         | '|.'                           | line 2, col 31\n",
      "  NODE_DEF        | 'out'                          | line 3, col 1\n",
      "  PIPE            | '|'                            | line 3, col 6\n",
      "  NODE_REF        | 'greeting'                     | line 3, col 7\n",
      "  DOT_END         | '|.'                           | line 3, col 16\n",
      "  EOF             | ''                             | line 4, col 1\n"
     ]
    }
   ],
   "source": [
    "# Simple hello world example\n",
    "source = '''\n",
    "@greeting |<<<Hello, World!>>>|.\n",
    "@out |$greeting|.\n",
    "'''\n",
    "\n",
    "tokens = tokenizer.tokenize(source)\n",
    "\n",
    "print(f\"Source: {len(source)} chars -> {len(tokens)} tokens\\n\")\n",
    "for token in tokens:\n",
    "    print(f\"  {token.type.name:15} | {token.value!r:30} | line {token.line}, col {token.column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379d913",
   "metadata": {},
   "source": [
    "## 3. Token Type Examples\n",
    "\n",
    "Let's explore each token type individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd80de",
   "metadata": {},
   "source": [
    "### 3.1 Comments (`#`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eadd44c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT         -> '# This is a comment'\n",
      "COMMENT         -> '# Comments are ignored by the parser'\n",
      "NODE_DEF        -> 'node'\n",
      "PIPE            -> '|'\n",
      "STRING_TRIM     -> 'Content'\n",
      "DOT_END         -> '|.'\n",
      "EOF             -> ''\n"
     ]
    }
   ],
   "source": [
    "source = '''\n",
    "# This is a comment\n",
    "# Comments are ignored by the parser\n",
    "@node |<<<Content>>>|.\n",
    "'''\n",
    "\n",
    "tokens = tokenizer.tokenize(source)\n",
    "for token in tokens:\n",
    "    print(f\"{token.type.name:15} -> {token.value!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24c237",
   "metadata": {},
   "source": [
    "### 3.2 Node Definitions and References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "141a32d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference tokens:\n",
      "  NODE_DEF        -> 'first'\n",
      "  NODE_DEF        -> 'second'\n",
      "  NODE_REF        -> 'first'\n",
      "  NODE_DEF        -> 'third'\n",
      "  FORWARD_REF     -> 'later'\n",
      "  NODE_DEF        -> 'later'\n"
     ]
    }
   ],
   "source": [
    "source = '''\n",
    "@first |<<<First node>>>|.\n",
    "@second |<<<Uses: >>>|$first|.\n",
    "@third |<<<Forward ref: >>>|^later|.\n",
    "@later |<<<Defined later>>>|.\n",
    "'''\n",
    "\n",
    "tokens = tokenizer.tokenize(source)\n",
    "print(\"Reference tokens:\")\n",
    "for token in tokens:\n",
    "    if token.type in [TokenType.NODE_DEF, TokenType.NODE_REF, TokenType.FORWARD_REF]:\n",
    "        print(f\"  {token.type.name:15} -> {token.value!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5decf3b",
   "metadata": {},
   "source": [
    "### 3.3 File References (`++path`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "090dcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reference tokens:\n",
      "  FILE_REF        -> './README.md'\n",
      "  FILE_REF        -> './config.yaml'\n"
     ]
    }
   ],
   "source": [
    "source = '''\n",
    "@docs |<<<Documentation: >>>|++./README.md|.\n",
    "@config |<<<Config: >>>|++./config.yaml|.\n",
    "'''\n",
    "\n",
    "tokens = tokenizer.tokenize(source)\n",
    "print(\"File reference tokens:\")\n",
    "for token in tokens:\n",
    "    if token.type == TokenType.FILE_REF:\n",
    "        print(f\"  {token.type.name:15} -> {token.value!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f77d30",
   "metadata": {},
   "source": [
    "### 3.4 Import Statements (`+path`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e21a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import tokens:\n",
      "  IMPORT          -> './common.flow'\n",
      "  IMPORT          -> './utils.flow'\n"
     ]
    }
   ],
   "source": [
    "source = '''\n",
    "+./common.flow |.\n",
    "+./utils.flow |$helper|.\n",
    "'''\n",
    "\n",
    "tokens = tokenizer.tokenize(source)\n",
    "print(\"Import tokens:\")\n",
    "for token in tokens:\n",
    "    if token.type == TokenType.IMPORT:\n",
    "        print(f\"  {token.type.name:15} -> {token.value!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29207f26",
   "metadata": {},
   "source": [
    "### 3.5 String Blocks (Trim vs Preserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53d7d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String block tokens:\n",
      "  STRING_TRIM     -> 'Trimmed whitespace'\n",
      "  STRING_PRESERVE -> '   Preserved whitespace   '\n"
     ]
    }
   ],
   "source": [
    "source = '''\n",
    "@trimmed |<<<   Trimmed whitespace   >>>|.\n",
    "@preserved |<<   Preserved whitespace   >>|.\n",
    "'''\n",
    "\n",
    "tokens = tokenizer.tokenize(source)\n",
    "print(\"String block tokens:\")\n",
    "for token in tokens:\n",
    "    if token.type in [TokenType.STRING_TRIM, TokenType.STRING_PRESERVE]:\n",
    "        print(f\"  {token.type.name:15} -> {token.value!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd65a",
   "metadata": {},
   "source": [
    "## 4. Tokenize Sample Files\n",
    "\n",
    "Let's tokenize some of the sample `.flow` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c8bc2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd: /home/stellar/PublicRepo/YMHY/anime_streamer_2_0/cores/flow_core\n",
      "=== hello.flow ===\n",
      "# Simple greeting node\n",
      "# This is the most basic FLOW file example\n",
      "\n",
      "@greeting |style.title=<<Hello>>|<<<Welcome to FLOW!>>>|.\n",
      "\n",
      "@out |$greeting|.\n",
      "\n",
      "\n",
      "=== Tokens ===\n",
      "  COMMENT         -> '# Simple greeting node'\n",
      "  COMMENT         -> '# This is the most basic FLOW file example'\n",
      "  NODE_DEF        -> 'greeting'\n",
      "  PIPE            -> '|'\n",
      "  IDENTIFIER      -> 'style.title'\n",
      "  ASSIGN          -> '='\n",
      "  STRING_PRESERVE -> 'Hello'\n",
      "  PIPE            -> '|'\n",
      "  STRING_TRIM     -> 'Welcome to FLOW!'\n",
      "  DOT_END         -> '|.'\n",
      "  NODE_DEF        -> 'out'\n",
      "  PIPE            -> '|'\n",
      "  NODE_REF        -> 'greeting'\n",
      "  DOT_END         -> '|.'\n",
      "  EOF             -> ''\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Read and tokenize the hello.flow sample\n",
    "sample_path = Path('./playground/samples/hello.flow')\n",
    "print(f\"pwd: {Path.cwd()}\")\n",
    "\n",
    "if sample_path.exists():\n",
    "    source = sample_path.read_text()\n",
    "    print(f\"=== {sample_path.name} ===\")\n",
    "    print(source)\n",
    "    print(\"\\n=== Tokens ===\")\n",
    "    tokens = tokenizer.tokenize(source)\n",
    "    for token in tokens:\n",
    "        print(f\"  {token.type.name:15} -> {token.value!r}\")\n",
    "else:\n",
    "    print(f\"Sample file not found: {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef929040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== refs.flow ===\n",
      "# Reference types demonstration\n",
      "# Shows $ref, ^forward ref, and ++file ref\n",
      "\n",
      "# Node reference ($) - references an existing node\n",
      "@greeting |<<<Hello!>>>|.\n",
      "\n",
      "@welcome |<<<Welcome message: >>>|$greeting|.\n",
      "\n",
      "# Forward reference (^) - references a node defined later\n",
      "@preview |<<<Coming up: >>>|^conclusion|.\n",
      "\n",
      "# File reference (++) - embeds external file content\n",
      "@resources |<<<Documentation: >>>|++./docs/guide.md|.\n",
      "\n",
      "# Mixed references in one node\n",
      "@combined |<<<Start>>>|$greeting|<<<Middle>>>|^ending|<<<Se...\n",
      "\n",
      "=== Token Summary ===\n",
      "  PIPE                 -> 16\n",
      "  STRING_TRIM          -> 9\n",
      "  COMMENT              -> 8\n",
      "  NODE_DEF             -> 8\n",
      "  DOT_END              -> 8\n",
      "  NODE_REF             -> 3\n",
      "  FORWARD_REF          -> 2\n",
      "  FILE_REF             -> 2\n",
      "  EOF                  -> 1\n"
     ]
    }
   ],
   "source": [
    "# Read and tokenize the refs.flow sample (demonstrates all reference types)\n",
    "sample_path = Path('./playground/samples/refs.flow')\n",
    "if sample_path.exists():\n",
    "    source = sample_path.read_text()\n",
    "    print(f\"=== {sample_path.name} ===\")\n",
    "    print(source[:500] + \"...\" if len(source) > 500 else source)\n",
    "    print(\"\\n=== Token Summary ===\")\n",
    "    tokens = tokenizer.tokenize(source)\n",
    "    \n",
    "    # Count by type\n",
    "    from collections import Counter\n",
    "    type_counts = Counter(t.type.name for t in tokens)\n",
    "    for token_type, count in type_counts.most_common():\n",
    "        print(f\"  {token_type:20} -> {count}\")\n",
    "else:\n",
    "    print(f\"Sample file not found: {sample_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174d846",
   "metadata": {},
   "source": [
    "## 5. Interactive Tokenizer\n",
    "\n",
    "Use this cell to experiment with your own Flow source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9efd94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: 9 tokens\n",
      "\n",
      "  NODE_DEF        | 'greeting'\n",
      "  PIPE            | '|'\n",
      "  STRING_TRIM     | 'Hello!'\n",
      "  DOT_END         | '|.'\n",
      "  NODE_DEF        | 'out'\n",
      "  PIPE            | '|'\n",
      "  NODE_REF        | 'greeting'\n",
      "  DOT_END         | '|.'\n",
      "  EOF             | ''\n"
     ]
    }
   ],
   "source": [
    "# Your FLOW source here\n",
    "source = '''\n",
    "@greeting |<<<Hello!>>>|.\n",
    "@out |$greeting|.\n",
    "'''\n",
    "\n",
    "try:\n",
    "    tokens = tokenizer.tokenize(source)\n",
    "    print(f\"Tokenized: {len(tokens)} tokens\\n\")\n",
    "    for token in tokens:\n",
    "        print(f\"  {token.type.name:15} | {token.value!r}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
